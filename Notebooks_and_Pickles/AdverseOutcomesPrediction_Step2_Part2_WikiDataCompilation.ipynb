{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Project: Adverse Medical Outcomes Prediction \n",
    "##### Data Scientist: Victoria M. Ng "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victoria/anaconda3/envs/ds/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n",
      "2018-05-11 07:02:53,015 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "/home/victoria/anaconda3/envs/ds/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n",
      "/home/victoria/anaconda3/envs/ds/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# General system libraries\n",
    "import os\n",
    "import sys\n",
    "from IPython.display import Image, Markdown\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Dataframe libraries\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, read_csv\n",
    "\n",
    "# Number manipulation\n",
    "import scipy.sparse\n",
    "from scipy.ndimage.filters import generic_filter\n",
    "import patsy\n",
    "import numpy as np\n",
    "\n",
    "# Plotting libaries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "# Data type libaries\n",
    "from datetime import datetime as dt\n",
    "\n",
    "# File manipulation\n",
    "import pickle\n",
    "import pandas.io.sql as pd_sql\n",
    "import psycopg2 as pg\n",
    "\n",
    "# NLP libraries\n",
    "import wikipedia as wiki\n",
    "from nltk import word_tokenize, sent_tokenize,FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "import gensim as gn\n",
    "from gensim import corpora, models, similarities\n",
    "from collections import defaultdict\n",
    "from six import iteritems\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# Scraping libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "chromedriver = \"/home/victoria/projects/metis/Project3/chromedriver\"\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "\n",
    "# Stats libaries\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn import datasets, linear_model, metrics\n",
    "from sklearn.cross_validation import cross_val_score, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Ridge, Lasso, RidgeCV, LinearRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "# Other libaries\n",
    "import geopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Import all subsets of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('/Corpus/full_corpus_200_fixed.pkl', 'rb') as picklefile: \n",
    "    full_corpus_200_fixed = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('/Corpus/full_corpus_400_fixed.pkl', 'rb') as picklefile: \n",
    "    full_corpus_400_fixed = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('/Corpus/full_corpus_600_fixed.pkl', 'rb') as picklefile: \n",
    "    full_corpus_600_fixed = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('/Corpus/full_corpus_800_fixed.pkl', 'rb') as picklefile: \n",
    "    full_corpus_800_fixed = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('/Corpus/full_corpus_1000_fixed.pkl', 'rb') as picklefile: \n",
    "    full_corpus_1000_fixed = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('/Corpus/full_corpus_1500_fixed.pkl', 'rb') as picklefile: \n",
    "    full_corpus_1500_fixed = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('/Corpus/full_corpus_2000_fixed.pkl', 'rb') as picklefile: \n",
    "    full_corpus_2000_fixed = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('/Corpus/full_corpus_2500_fixed.pkl', 'rb') as picklefile: \n",
    "    full_corpus_2500_fixed = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('/Corpus/full_corpus_3000_fixed_again.pkl', 'rb') as picklefile: \n",
    "    full_corpus_3000_fixed_again = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('/Corpus/full_corpus_remainder.pkl', 'rb') as picklefile: \n",
    "    full_corpus_remainder = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Concatenate all subsets of the corpus into one full corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wiki_symptom_full_corpus = full_corpus_200_fixed + full_corpus_400_fixed + full_corpus_600_fixed + full_corpus_800_fixed + full_corpus_1000_fixed + full_corpus_1500_fixed + full_corpus_2000_fixed + full_corpus_2500_fixed + full_corpus_3000_fixed_again + full_corpus_remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3004"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wiki_symptom_full_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('wiki_symptom_full_corpus.pkl', 'wb') as picklefile:\n",
    "        pickle.dump(wiki_symptom_full_corpus, picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Vectorize the full wiki corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stop_words = list(STOP_WORDS)\n",
    "stop_words.append('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cv_wiki_symptom_full_corpus = CountVectorizer(lowercase=True, stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=['even', 'between', 'otherwise', 'somehow', 'keep', 'hereupon', 'become', 'being', 'please', 'former', 'they', 'within', 'quite', 'say', 'was', 'sixty', 'fifty', 'perhaps', 'further', 'his', 'all', 'too', 'whereafter', 'last', 'ourselves', 'a', 'by', 'how', 'would', 'as', 'together', 'bec...will', 'rather', 'unless', 'next', 'never', 'except', 'besides', 'much', 'per', 'before', 'of', 'a'],\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_wiki_symptom_full_corpus.fit(wiki_symptom_full_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139329"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_symptom_full_corpus_dict = cv_wiki_symptom_full_corpus.vocabulary_\n",
    "len(wiki_symptom_full_corpus_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45612"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_symptom_full_corpus_dict.get('feces', 'Nope')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wiki_symptom_full_corpus_vect = cv_wiki_symptom_full_corpus.transform(wiki_symptom_full_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Apply PCA to reduce the number of columns( words) that will become the bins (categories ) for my KNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pca_20_wiki_symptom_full_corpus = PCA(n_components = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=20, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_20_wiki_symptom_full_corpus.fit(wiki_symptom_full_corpus_vect.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# matrix with 20 most ~'common' combinations of words\n",
    "wiki_symptom_full_corpus_20_dim_matrix = pca_20_wiki_symptom_full_corpus.transform(wiki_symptom_full_corpus_vect.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wiki_symptom_full_corpus_20_dim_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3004, 20)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_symptom_full_corpus_20_dim_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Look at interesting attributes of the KNN model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06438179 0.0311082  0.02742774 0.02549617 0.02085938 0.01899499\n",
      " 0.01328094 0.01229413 0.01068638 0.01011975 0.00972889 0.00919736\n",
      " 0.00884447 0.0081622  0.0079821  0.00793626 0.00765372 0.00740921\n",
      " 0.00721613 0.00702847]\n"
     ]
    }
   ],
   "source": [
    "print(pca_20_wiki_symptom_full_corpus.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1501.3102963  1043.58157987  979.90492598  944.77073001  854.55433267\n",
      "  815.4708309   681.87284181  656.05122916  611.65176393  595.21491583\n",
      "  583.60726721  567.4406782   556.44821252  534.55517032  528.6248833\n",
      "  527.10479107  517.63683446  509.30145278  502.62158321  496.04295962]\n"
     ]
    }
   ],
   "source": [
    "print(pca_20_wiki_symptom_full_corpus.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.05397482e-04 -7.73336821e-06  9.69304006e-05 ...  9.47439348e-05\n",
      "   7.28799498e-06  7.28799498e-06]\n",
      " [ 8.98259809e-04 -1.65733056e-05  1.97098597e-05 ... -4.90547406e-05\n",
      "  -3.77344159e-06 -3.77344159e-06]\n",
      " [ 1.76587484e-04 -3.44689238e-05 -4.75878317e-05 ... -1.34368140e-04\n",
      "  -1.03360107e-05 -1.03360107e-05]\n",
      " ...\n",
      " [-1.12422923e-04  9.65011374e-05  1.98605043e-04 ...  3.45732052e-04\n",
      "   2.65947732e-05  2.65947732e-05]\n",
      " [ 6.27931967e-05 -2.28074793e-04 -2.04487902e-04 ...  3.24378432e-04\n",
      "   2.49521870e-05  2.49521870e-05]\n",
      " [ 3.61698582e-04  3.08833923e-05  8.59353467e-05 ... -6.16059203e-04\n",
      "  -4.73891695e-05 -4.73891695e-05]]\n"
     ]
    }
   ],
   "source": [
    "print(pca_20_wiki_symptom_full_corpus.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Test how you're going to get your test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-17.91325639,  -2.35474632,  -0.12414797, ...,   0.97728442,\n",
       "          1.441557  ,   0.46093553],\n",
       "       [-19.19324951,  -3.60991213,  -0.57976562, ...,   3.29253031,\n",
       "         -1.24793116,   0.46976203],\n",
       "       [-18.63415974,  -2.11638159,   1.01150961, ...,   0.57562599,\n",
       "         -0.93591457,   0.15758927],\n",
       "       ...,\n",
       "       [-15.80131469,   3.85749768,   2.49505228, ...,  -1.07114407,\n",
       "         -1.91754479,   1.33740113],\n",
       "       [  1.19297458,  -9.78163688,   5.20028238, ...,  -5.12938109,\n",
       "         -2.34153901,  -7.61357924],\n",
       "       [ 32.30433105, -10.47030214, -23.96821249, ...,   1.43799505,\n",
       "          0.88625508, -16.34231153]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_symptom_full_corpus_20_dim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wiki_symptom_full_corpus_20_dim_matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.91932495e+01, -3.60991213e+00, -5.79765617e-01,\n",
       "         1.93507417e+00, -1.22329064e+00,  3.19618122e+00,\n",
       "        -6.26147046e-01, -7.76237818e-04, -1.62891795e+00,\n",
       "         1.74167825e+00,  5.50904117e-01, -2.53897081e+00,\n",
       "         2.56192110e+00, -1.60691884e+00, -1.66295676e-01,\n",
       "         2.31988715e+00, -1.96352069e+00,  3.29253031e+00,\n",
       "        -1.24793116e+00,  4.69762028e-01],\n",
       "       [-1.86341597e+01, -2.11638159e+00,  1.01150961e+00,\n",
       "         1.30934891e+00, -1.51075949e+00,  6.02667167e+00,\n",
       "         7.45225390e-01,  4.99584925e-02, -1.29999190e+00,\n",
       "         2.01529449e+00,  1.64704970e-01, -7.03651107e-01,\n",
       "        -1.12247023e+00, -3.70048853e-01, -9.00026923e-01,\n",
       "         1.09761547e+00,  1.42795547e-01,  5.75625990e-01,\n",
       "        -9.35914571e-01,  1.57589269e-01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_a = wiki_symptom_full_corpus_20_dim_matrix[1]\n",
    "test_b = wiki_symptom_full_corpus_20_dim_matrix[2]\n",
    "test_final = np.array([test_a, test_b])\n",
    "test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Create the test and train corpus matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Below are the bins I want to categorize my symptoms into and their corresponding index in the unique symptoms list. I tried to get a combination of both severe symptoms and common symptoms. I also tried to find symptoms that were generalizable but not too much that there can be both severe and common reactions mapped to the same symptom.\n",
    "1. Diarrhea (69)\n",
    "2. Mood_swing (104)\n",
    "3. Renal function (859)\n",
    "4. Upper respiratory tract infection (904)\n",
    "5. Prothrombin time (1013) \n",
    "6. Bleeding (1141)\n",
    "7. Nasal congestion (1261)\n",
    "8. Drug overdose (1297)\n",
    "9. Angina (1341)\n",
    "10. Dysbiosis (1873)\n",
    "11. Overactive bladder (2359)\n",
    "12. Suicide terminology (2695)\n",
    "13. Epileptic seizure (2843)\n",
    "14. Nephrotoxicity (2953)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Diarrhea = wiki_symptom_full_corpus_20_dim_matrix[69]\n",
    "Mood_swing = wiki_symptom_full_corpus_20_dim_matrix[104]\n",
    "Renal_function = wiki_symptom_full_corpus_20_dim_matrix[859]\n",
    "Upper_respiratory_tract_infection = wiki_symptom_full_corpus_20_dim_matrix[904]\n",
    "Prothrombin_time = wiki_symptom_full_corpus_20_dim_matrix[1013]\n",
    "Bleeding = wiki_symptom_full_corpus_20_dim_matrix[1141]\n",
    "Nasal_congestion = wiki_symptom_full_corpus_20_dim_matrix[1261]\n",
    "Drug_overdose = wiki_symptom_full_corpus_20_dim_matrix[1297]\n",
    "Angina = wiki_symptom_full_corpus_20_dim_matrix[1341]\n",
    "Dysbiosis = wiki_symptom_full_corpus_20_dim_matrix[1873]\n",
    "Overactive_bladder = wiki_symptom_full_corpus_20_dim_matrix[2359]\n",
    "Suicide_terminology = wiki_symptom_full_corpus_20_dim_matrix[2695]\n",
    "Epileptic_seizure = wiki_symptom_full_corpus_20_dim_matrix[2843]\n",
    "Nephrotoxicity = wiki_symptom_full_corpus_20_dim_matrix[2953]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_indices = [69, 104, 859, 904, 1013, 1141, 1261, 1297, 1341, 1873, 2359, 2695, 2843, 2953]\n",
    "len(target_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mask = np.zeros(len(wiki_symptom_full_corpus_20_dim_matrix), dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mask[target_indices] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train = wiki_symptom_full_corpus_20_dim_matrix[target_indices]\n",
    "y_train = [x for x in range(len(target_indices))]\n",
    "\n",
    "X_test = wiki_symptom_full_corpus_20_dim_matrix[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 20)\n",
      "14\n",
      "(2990, 20)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(len(y_train))\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Pickle the test train split for feature engineering purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('wiki_x_train.pkl', 'wb') as picklefile:\n",
    "        pickle.dump(X_train, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('wiki_y_train.pkl', 'wb') as picklefile:\n",
    "        pickle.dump(y_train, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('wiki_x_test.pkl', 'wb') as picklefile:\n",
    "        pickle.dump(X_test, picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### What I did\n",
    "1. Concatenated all subsets of the Wikipedia corpus into one full corpus\n",
    "2. Vectorized the corpus\n",
    "3. Applied PCA to reduce the dimensionality of my matrix\n",
    "4. Created the test and train matrices\n",
    "\n",
    "### What I will do next\n",
    "1. Apply a KNN N=1 model to the reduced dimension Wikipedia corpus matrix to categorize all symptoms for purposes of making them my features in my final model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
