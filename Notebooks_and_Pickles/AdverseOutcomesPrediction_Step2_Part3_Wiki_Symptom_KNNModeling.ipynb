{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Project: Adverse Medical Outcomes Prediction \n",
    "##### Data Scientist: Victoria M. Ng "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victoria/anaconda3/envs/ds/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n",
      "2018-05-11 15:28:09,780 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "/home/victoria/anaconda3/envs/ds/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n",
      "/home/victoria/anaconda3/envs/ds/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# General system libraries\n",
    "import os\n",
    "import sys\n",
    "from IPython.display import Image, Markdown\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Dataframe libraries\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, read_csv\n",
    "\n",
    "# Number manipulation\n",
    "import scipy.sparse\n",
    "from scipy.ndimage.filters import generic_filter\n",
    "import patsy\n",
    "import numpy as np\n",
    "\n",
    "# Plotting libaries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "# Data type libaries\n",
    "from datetime import datetime as dt\n",
    "\n",
    "# File manipulation\n",
    "import pickle\n",
    "import pandas.io.sql as pd_sql\n",
    "import psycopg2 as pg\n",
    "\n",
    "# NLP libraries\n",
    "import wikipedia as wiki\n",
    "from nltk import word_tokenize, sent_tokenize,FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "import gensim as gn\n",
    "from gensim import corpora, models, similarities\n",
    "from collections import defaultdict\n",
    "from six import iteritems\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# Scraping libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "chromedriver = \"/home/victoria/projects/metis/Project3/chromedriver\"\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "\n",
    "# Stats libaries\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn import datasets, linear_model, metrics\n",
    "from sklearn.cross_validation import cross_val_score, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Ridge, Lasso, RidgeCV, LinearRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "# Other libaries\n",
    "import geopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Import test train split of vectorized and PCA'ed wiki symptom corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('wiki_x_train.pkl', 'rb') as picklefile: \n",
    "    X_train = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('wiki_y_train.pkl', 'rb') as picklefile: \n",
    "    y_train = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('wiki_x_test.pkl', 'rb') as picklefile: \n",
    "    X_test = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('wiki_symptom_full_corpus.pkl', 'rb') as picklefile: \n",
    "    wiki_symptom_full_corpus = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Review the bins I chose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Below are the bins I want to categorize my symptoms into and their corresponding index in the unique symptoms list. I tried to get a combination of both severe symptoms and common symptoms. I also tried to find symptoms that were generalizable but not too much that there can be both severe and common reactions mapped to the same symptom.\n",
    "1. Diarrhea (69)\n",
    "2. Mood_swing (104)\n",
    "3. Renal function (859)\n",
    "4. Upper respiratory tract infection (904)\n",
    "5. Prothrombin time (1013) \n",
    "6. Bleeding (1141)\n",
    "7. Nasal congestion (1261)\n",
    "8. Drug overdose (1297)\n",
    "9. Angina (1341)\n",
    "10. Dysbiosis (1873)\n",
    "11. Overactive bladder (2359)\n",
    "12. Suicide terminology (2695)\n",
    "13. Epileptic seizure (2843)\n",
    "14. Nephrotoxicity (2953)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Fit KNN K=1model to the training matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "KNN_symptoms = KNeighborsClassifier(n_neighbors=1, n_jobs=-1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=-1, n_neighbors=1, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN_symptoms.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Get the categorizations (predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predictions = KNN_symptoms.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 82, 117,  36, 303, 381,  97, 379,  61,  90, 743, 136,  96, 262,\n",
       "       207])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the distribution of the docs to bins\n",
    "np.bincount(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 9, 9, ..., 6, 3, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('symptoms_categorizations.pkl', 'wb') as picklefile:\n",
    "        pickle.dump(predictions, picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Create a lookup table for all of the symptoms to their respective bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "target_indices = [69, 104, 859, 904, 1013, 1141, 1261, 1297, 1341, 1873, 2359, 2695, 2843, 2953]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('symptoms_to_wiki_search_list.pkl', 'rb') as picklefile: \n",
    "    symptoms_to_wiki_search_list = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "swelling face\n"
     ]
    }
   ],
   "source": [
    "symptoms_to_bin = list(np.delete(symptoms_to_wiki_search_list,[target_indices]))\n",
    "print(type(symptoms_to_bin))\n",
    "print(symptoms_to_bin[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'symptoms_to_bin' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5335681f1d4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlookup_symptoms_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymptoms_to_bin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlookup_symptoms_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mood altered'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'symptoms_to_bin' is not defined"
     ]
    }
   ],
   "source": [
    "lookup_symptoms_dict = dict(zip(symptoms_to_bin, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Pickle the lookup dict for wiki symptom feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('lookup_symptoms_dict.pkl', 'wb') as picklefile:\n",
    "        pickle.dump(lookup_symptoms_dict, picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### What I did\n",
    "1. Fit a KNN N=1 model to the training matrix of the Wikipedia corpus\n",
    "2. Get the categorizations (or predictions) for the symptoms that needed to be binned \n",
    "3. Created a lookup table to map the symptoms to their category(bin) \n",
    "\n",
    "### What I will do next\n",
    "1. Update the values of the symptoms bins within the symptoms matrix per row"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
